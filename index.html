<!DOCTYPE HTML>
<html lang="en">

<head>    
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-0B5LHBSZYY"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-0B5LHBSZYY');
  </script>

  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Peiyang Song</title>

  <meta name="author" content="Peiyang Song">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="apple-touch-icon" sizes="180x180" href="images/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="images/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="images/favicon-16x16.png">
  <link rel="manifest" href="images/site.webmanifest">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
</head>

<body>
  <table style="width:100%;max-width:1050px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:1.5%;width:70%;vertical-align:middle">
              <p style="text-align:center">
                <name>Peiyang Song</name>
              </p>
              <p>
                I am a 4th-year undergrad studying <b>Computer Science</b> at California Institute of Technology (<b>Caltech</b>), advised by Prof. <a style="text-decoration: none" target="_blank" href="https://netlab.caltech.edu/">Steven Low</a>, with a minor in <b>Robotics</b> advised by Prof. <a style="text-decoration: none" target="_blank" href="https://scholar.google.com/citations?user=-YP8MJ0AAAAJ&hl=en">Günter Niemeyer</a>. 
                I am in Berkeley AI Research (<b>BAIR</b>) Lab, advised by Prof. <a style="text-decoration: none" target="_blank" href="https://dawnsong.io/">Dawn Song</a> and Dr. <a style="text-decoration: none" target="_blank" href="https://jxhe.info/">Jingxuan He</a>. 
                I also work in Stanford AI Lab (<b>SAIL</b>), advised by Prof. <a style="text-decoration: none" target="_blank" href="https://cocolab.stanford.edu/ndg.html">Noah Goodman</a> and Dr. <a style="text-decoration: none" target="_blank" href="https://gpoesia.com/">Gabriel Poesia</a>. 
                I have been fortunate to work with Prof. <a style="text-decoration: none" target="_blank" href="http://tensorlab.cms.caltech.edu/users/anima/">Anima Anandkumar</a> (Caltech), Dr. <a style="text-decoration: none" target="_blank" href="https://yangky11.github.io/">Kaiyu Yang</a> (Meta), Prof. <a style="text-decoration: none" target="_blank" href="https://www.arch.cs.ucsb.edu/prof-sherwood">Tim Sherwood</a> (UC Santa Barbara), and Dr. <a style="text-decoration: none" target="_blank" href="https://dl.acm.org/profile/81100206077">Jeremy Lau</a> (Google) during my undergrad. 
              </p>
              <p>
                <font color="red"><strong>I am currently applying for 26Fall PhD positions in Computer Science. Please feel free to explore my research interest and work below. Would love to connect if you see potential fits!</strong></font>
              </p>
              <p style="text-align:center">
                宋沛洋 &nbsp/&nbsp
                <a target="_blank" href="mailto:psong@caltech.edu">Email</a> &nbsp/&nbsp
                <a target="_blank" href="data/Peiyang_Song_CV.pdf">CV</a> &nbsp/&nbsp
                <a target="_blank" href="data/Bio.txt">Bio</a> &nbsp/&nbsp
                <a target="_blank" href="https://scholar.google.com/citations?user=E1j11NQAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a target="_blank" href="https://github.com/Peiyang-Song">GitHub</a> &nbsp/&nbsp
                <a target="_blank" href="https://www.linkedin.com/in/peiyang-song-3279b3251/">LinkedIn</a> &nbsp/&nbsp
                <a target="_blank" href="https://twitter.com/p_song1">Twitter</a>
              </p>
            </td>
            <td style="padding:2.5%;width:33%;max-width:33%">
              <a href="images/Peiyang Song.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/Peiyang Song - Circle.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Recent News</heading>
              <p><b>[Sep 2025]</b>  We released our work discovering <a href="https://psychology-of-ai.github.io/">The Personality Illusion</a>: LLMs do not have personalities in the way humans do.<br>
                 <b>[Jul 2025]</b>  Our paper on <a href="https://ieeexplore.ieee.org/abstract/document/11082679">Delay Space Arithmetic and Architecture</a> is awarded IEEE Micro <b>Top Picks</b>.<br>
                 <b>[Jul 2025]</b>  I will be at the Agentic AI Summit 2025 on August 2 in UC Berkeley. Register <a href="https://rdi.berkeley.edu/events/agentic-ai-summit">here</a>. Excited to catch up there!<br>
                 <b>[Jul 2025]</b>  Our paper on <a href="https://openreview.net/forum?id=hsgMn4KBFG">LLM Reasoning Failures</a> is accepted to <a href="https://sites.google.com/view/ai4mathworkshopicml2025/">ICML AI for Math Workshop</a>. Stay tuned for our full release!<br>
                 <b>[Jun 2025]</b>  Our paper on <a href="https://openreview.net/forum?id=pdLNGgdO1A">Human-Like Traits in LLMs</a> is accepted to <a href="https://sites.google.com/view/mhf-icml2025">ICML MoFA Workshop</a>. Stay tuned for our full release!<br>
<!--                  <b>[Jun 2025]</b>  I am back in the bay this summer from mid-June to late-September. Hit me up if you are around!<br> -->
<!--                  <b>[Jun 2025]</b>  I am featured by Caltech in the <a target="_blank" href="https://www.eas.caltech.edu/community/vibrations">Division of Engineering and Applied Science (EAS) Newsletter Spotlight</a>. Thank you!<br> -->
<!--                  <b>[May  2025]</b>  Attending <a href="https://neus-2025.github.io">NeuS 2025</a> in Philadelphia, PA from 5/27 to 5/30, presenting <a href="https://arxiv.org/abs/2404.12534">Lean Copilot</a>.<br> -->
<!--                  <b>[May  2025]</b>  Honored to be recognized by ICLR 2025 as a <a href="https://iclr.cc/Conferences/2025/Reviewers">notable reviewer</a>.<br> -->
<!--                  <b>[May  2025]</b>  I am hired by the <a href="https://www.admissions.caltech.edu/">Caltech Undergraduate Admissions Office</a> as Admissions Ambassador.<br> -->
<!--                  <b>[Apr 2025]</b>  Honored to receive the <a href="https://deans.caltech.edu/Grants_Funding/gwhfund">George W. Housner Student Discovery Fund</a> at Caltech.<br> -->
<!--                  <b>[Apr 2025]</b>  Our paper <a href="https://arxiv.org/abs/2404.12534">Lean Copilot</a> is accepted to <a href="https://neus-2025.github.io">NeuS 2025</a>. See you in Philadelphia, PA!<br> -->
<!--                  <b>[Mar 2025]</b>  I am joining Prof. <a href="https://dawnsong.io/">Dawn Song</a>'s group at UC Berkeley starting June 2025, working on AI for formal math and verifiable code.<br> -->
<!--                  <b>[Mar 2025]</b>  Our paper <a href="https://arxiv.org/abs/2502.17925">LeanProgress</a> is accepted to ICLR 2025 VerifAI Workshop. See you in Singapore!<br> -->
<!--                  <b>[Feb 2025]</b>  Our ASPLOS 2024 paper <a href="https://dl.acm.org/doi/10.1145/3620665.3640395">Energy Efficient Convolutions with Temporal Arithmetic</a> is awarded IEEE Micro Top Pick 2025.<br> -->
<!--                  <b>[Nov 2024]</b>  Thanks <a target="_blank" href="https://www.scientificamerican.com/article/mathematicians-newest-assistants-are-artificially-intelligent/">Scientific American</a> for featuring my works on AI for mathematical reasoning, including <a target="_blank" href="https://arxiv.org/abs/2306.15626">LeanDojo</a> and <a target="_blank" href="https://arxiv.org/abs/2404.12534">Lean Copilot</a>.<br> -->
<!--                  <b>[Sep 2024]</b>  Our paper <a href="https://arxiv.org/abs/2410.00988">Creative and Context-Aware Translation of East Asian Idioms with GPT-4</a> is accepted to EMNLP 2024 Findings.<br> -->
<!--                  <b>[Sep 2024]</b>  Our paper on <a href="https://arxiv.org/abs/2409.15454">LLM inhibitory control & A-not-B cognitive errors</a> is accepted to EMNLP 2024 Findings.<br> -->
<!--                  <b>[Sep 2024]</b>  I am giving an invited tutorial at <a href="https://neurosymbolic.github.io/nsss2024/">NSSS 2024</a> on Neuro-Symbolic Theorem Proving with Lean: <a href="data/Neuro-Symbolic Theorem Proving with Lean.pdf">slides</a>.<br> -->
<!--                  <b>[Aug 2024]</b>  Our paper on <a href="https://openreview.net/pdf?id=qu6sMJmEwl">trustworthy LLM reasoning in ICL</a> is accepted to ICML 2024 Workshop on LLMs and Cognition.<br> -->
<!--                  <b>[Jun 2024]</b>  I am joining <a href="https://ai.stanford.edu/">Stanford AI Lab (SAIL)</a> and <a href="https://cocolab.stanford.edu/">CoCoLab</a>, working on mathematical reasoning with LLMs.<br> -->
<!--                  <b>[May  2024]</b>  Attending <a href="https://www.neusconference.org/">NeuS</a> at Berkeley, CA.<br> -->
<!--                  <b>[Feb 2024]</b>  Our paper <a href="https://dl.acm.org/doi/10.1145/3620665.3640395">Energy Efficient Convolutions with Temporal Arithmetic</a> is accepted to ASPLOS 2024.<br> -->
<!--                  <b>[Dec 2023]</b>  Attending <a href="https://nips.cc/">NeurIPS</a> at New Orleans, LA, presenting <a href="https://leandojo.org">LeanDojo</a> and <a href="https://https://github.com/lean-dojo/LeanCopilot">Lean Copilot</a>.<br> -->
<!--                  <b>[Nov 2023]</b>  Our paper <a href="https://https://github.com/lean-dojo/LeanCopilot">Lean Copilot</a> is accepted to NeurIPS 2023 MATH-AI Workshop.<br> -->
<!--                  <b>[Sep 2023]</b>  Our paper <a href="https://leandojo.org">LeanDojo</a> is accepted to NeurIPS 2023 Datasets and Benchmarks Track as an Oral Presentation.<br> -->
<!--                  <b>[Sep 2023]</b>  We released <a target="_blank" href="https://github.com/lean-dojo/LeanCopilot">Lean Copilot</a> for LLMs to act as copilots for theorem proving in Lean: <a target="_blank" href="https://youtu.be/OxFcZU5ihBk">Video demo</a>.<br> -->
<!--                  <b>[Aug 2023]</b>  Honored to receive the prestigious <a target="_blank" href="https://ersp.cs.ucsb.edu/home">Early Research Scholarship</a>!<br> -->
<!--                  <b>[Jun 2023]</b>  We released <a href="https://leandojo.org">LeanDojo</a>, an open-source playground for LLMs to prove formal theorems in Lean.<br> -->
<!--                  <b>[May  2023]</b>  I am joining Caltech <a target="_blank" href="http://tensorlab.cms.caltech.edu/users/anima/">Anima AI + Science Lab</a>, working on neuro-symbolic reasoning and more!<br> -->
<!--                  <b>[Apr 2023]</b>  My research received the prestigious <a target="_blank" href="https://sfp.caltech.edu/undergraduate-research/programs/surf">Caltech SFP SURF Award</a>!<br> -->
<!--                  <b>[Feb 2023]</b>  I am joining <a target="_blank" href="https://www.arch.cs.ucsb.edu/">UCSB ArchLab</a>, working on energy-efficient machine learning and more!</p> -->
            </td>
          </tr>
		
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>My research centers on <strong>LLM reasoning</strong>, especially for <strong>formal math</strong> and <strong>verifiable code</strong>, through <strong>neuro-symbolic</strong> approaches combining LLMs with formal systems such as <a target="_blank" href="https://lean-lang.org/">Lean</a>. This agenda unfolds along three interconnected directions:</p>
              <ul>
                <li>LLMs for proving formal <strong>math</strong> and generating verifiable <strong>code</strong> <a href="#leandojo">[LeanDojo]</a>, in a practical <a href="#leancopilot">[Lean Copilot]</a>, predictable <a href="#leanprogress">[LeanProgress]</a>, and extendable <a href="#leanagent">[LeanAgent]</a> manner.</li>
                <li>Trustworthy <strong>LLM</strong> (informal) <strong>reasoning</strong> in natural languages, motivated by cognitive principles <a href="#personality">[LLM Personality]</a> and limitation studies <a href="#a_not_b">[A-Not-B]</a> <a href="#a_not_b">[LLMRF]</a>.</li>
                <li>The application of <strong>neuro-symbolic</strong> approaches to <strong>fundamental</strong> challenges, such as energy-efficient inference <a href="#delaynet">[DelayNet]</a> <a href="#delayspace">[Delay Space]</a> and idiom translation <a href="#idiom">[Idioms]</a>.</li>
              </ul>
              <p>If any of this resonates with your interests, feel free to reach out and let's connect/collaborate!</p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr onmouseout="llmrf_stop()" onmouseover="llmrf_start()" id="llmrf">
            <td style="padding:5px;width:17%;vertical-align:middle">
              <div class="one">
                <div class="two" id='llmrf_image'>
                  <img src='images/llmrf-1.png' width="150">
                </div>
                <img src='images/llmrf-2.png' width="150">
              </div>
              <script type="text/javascript">
                function llmrf_start() {
                  document.getElementById('llmrf_image').style.opacity = "1";
                }

                function llmrf_stop() {
                  document.getElementById('llmrf_image').style.opacity = "0";
                }
                llmrf_stop()
              </script>
            </td>
            <td style="padding:20px;width:83%;vertical-align:middle">
              <a target="_blank" href="https://openreview.net/forum?id=hsgMn4KBFG">
                <papertitle>A Survey on Large Language Model Reasoning Failures</papertitle>
              </a>
              <br />
              <strong>Peiyang Song</strong>*, <a target="_blank" href="https://pengrui-han.github.io/">Pengrui Han</a>*, and <a target="_blank" href="https://cocolab.stanford.edu/ndg">Noah Goodman</a> (* <strong>Equal Contribution</strong>)
              <br />
              <em>ICML AI for Math Workshop</em>, 2025
              <br />
              <a target="_blank" href="https://openreview.net/forum?id=hsgMn4KBFG">preprint</a>
              /
              full release coming soon
              <p></p>
              <p>
                We present the first comprehensive survey dedicated to reasoning failures in LLMs. By unifying fragmented research efforts, our survey provides a structured perspective on systemic weaknesses in LLM reasoning, offering valuable insights and guiding future research towards building stronger, more reliable, and robust reasoning capabilities.
              </p>
            </td>
          </tr>

          <tr onmouseout="personality_stop()" onmouseover="personality_start()" id="personality">
            <td style="padding:5px;width:17%;vertical-align:middle">
              <div class="one">
                <div class="two" id='personality_image'>
                  <img src='images/personality-1.png' width="150">
                </div>
                <img src='images/personality-2.png' width="150">
              </div>
              <script type="text/javascript">
                function personality_start() {
                  document.getElementById('personality_image').style.opacity = "1";
                }

                function personality_stop() {
                  document.getElementById('personality_image').style.opacity = "0";
                }
                personality_stop()
              </script>
            </td>
            <td style="padding:20px;width:83%;vertical-align:middle">
              <a target="_blank" href="https://psychology-of-ai.github.io/">
                <papertitle>The Personality Illusion: Revealing Dissociation Between Self-Reports & Behavior in LLMs</papertitle>
              </a>
              <br />
              <a target="_blank" href="https://pengrui-han.github.io/">Pengrui Han</a>*, <a target="_blank" href="https://www.rkocielnik.com/">Rafal D. Kocielnik</a>*, <strong>Peiyang Song</strong>, <a target="_blank" href="https://www.collectivedesign.group.cam.ac.uk/">Ramit Debnath</a>, <a style="text-decoration: none" target="_blank" href="https://www.deanmobbslab.com/people">Dean Mobbs</a>, <a style="text-decoration: none" target="_blank" href="https://www.eas.caltech.edu/people/anima">Anima Anandkumar</a>, and <a style="text-decoration: none" target="_blank" href="https://www.rmichaelalvarez.com/">R. Michael Alvarez</a> (* <strong>Equal Contribution</strong>)
              <br />
              <em>ICML Workshop on Models of Human Feedback for AI Alignment (MoFA)</em>, 2025
              <br />
              <a target="_blank" href="https://arxiv.org/abs/2509.03730">arXiv</a>
              /
              <a target="_blank" href="https://psychology-of-ai.github.io/">project</a>
              /
              <a target="_blank" href="https://github.com/psychology-of-AI/Personality-Illusion">code</a>
              <p></p>
              <p>
                LLMs say they have personalities, but they don’t act like it. Alignment today shapes language, not behavior. This linguistic–behavioral dissociation cautions against equating coherent self-reports with cognitive depth. 
              </p>
            </td>
          </tr>

          <tr onmouseout="delayspace_stop()" onmouseover="delayspace_start()" id="delayspace">
            <td style="padding:5px;width:17%;vertical-align:middle">
              <div class="one">
                <div class="two" id='delayspace_image'>
                  <img src='images/delayspace-1.png' width="150">
                </div>
                <img src='images/delayspace-2.png' width="150">
              </div>
              <script type="text/javascript">
                function delayspace_start() {
                  document.getElementById('delayspace_image').style.opacity = "1";
                }

                function delayspace_stop() {
                  document.getElementById('delayspace_image').style.opacity = "0";
                }
                delayspace_stop()
              </script>
            </td>
            <td style="padding:20px;width:83%;vertical-align:middle">
              <a target="_blank" href="https://ieeexplore.ieee.org/abstract/document/11082679">
                <papertitle>Delay Space Arithmetic and Architecture</papertitle>
              </a>
              <br />
              <a target="_blank" href="https://www.linkedin.com/in/rhys-gretsch-462a951ab">Rhys Gretsch</a>, <strong>Peiyang Song</strong>, <a target="_blank" href="https://ieeexplore.ieee.org/author/38529297900">Advait Madhavan</a>, <a style="text-decoration: none" target="_blank" href="https://dl.acm.org/profile/81100206077">Jeremy Lau</a>, and <a style="text-decoration: none" target="_blank" href="https://www.arch.cs.ucsb.edu/prof-sherwood">Tim Sherwood</a>
              <br />
              <em>IEEE Micro</em>, 2025, <font color="red"><strong>Top Pick Award</strong></font>
              <br />
              <a target="_blank" href="https://ieeexplore.ieee.org/abstract/document/11082679">proceeding</a>
              <p></p>
              <p>
                What operations can you perform efficiently when you use the “time of arrival” of a signal’s edge to represent a number? We present negative-logarithmic delay space arithmetic as a completely new approach to temporal coding. Under this approach, general purpose arithmetic is transformed to a “soft” version of the standard temporal operations in such a way that preserves all of the algebraic identities.
              </p>
            </td>
          </tr>

          <tr onmouseout="leanprogress_stop()" onmouseover="leanprogress_start()" id="leanprogress">
            <td style="padding:5px;width:17%;vertical-align:middle">
              <div class="one">
                <div class="two" id='leanprogress_image'>
                  <img src='images/leanprogress-1.png' width="160">
                </div>
                <img src='images/leanprogress-2.png' width="160">
              </div>
              <script type="text/javascript">
                function leanprogress_start() {
                  document.getElementById('leanprogress_image').style.opacity = "1";
                }

                function leanprogress_stop() {
                  document.getElementById('leanprogress_image').style.opacity = "0";
                }
                leanprogress_stop()
              </script>
            </td>
            <td style="padding:20px;width:83%;vertical-align:middle">
              <a target="_blank" href="https://leandojo.org/leanprogress.html">
                <papertitle>LeanProgress: Guiding Search for Neural Theorem Proving via Proof Progress Prediction</papertitle>
              </a>
              <br />
              <a target="_blank" href="https://hsz0403.github.io/">Suozhi Huang</a>, <strong>Peiyang Song</strong>, <a target="_blank" href="https://www.robertj1.com/">Robert Joseph George</a>, and <a target="_blank" href="https://tensorlab.cms.caltech.edu/users/anima/">Anima Anandkumar</a>
              <br />
              <em>ICLR VerifAI: AI Verification in the Wild Workshop</em>, 2025
              <br />
              <a target="_blank" href="https://arxiv.org/abs/2502.17925">arXiv</a>
			  /
			  <a target="_blank" href="https://leandojo.org/leanprogress.html">project</a>
              <p></p>
              <p>
                LLMs struggling with long proofs? We present LeanProgress, which uses a novel critic model where “distance” to the goal state acts as a key signal for step prediction, boosting performance on neural theorem proving in Lean. Our method achieves 75.1% prediction accuracy, with a 3.8% gain in proof search with step prediction.
              </p>
            </td>
          </tr>

          <tr onmouseout="leanagent_stop()" onmouseover="leanagent_start()" id="leanagent">
            <td style="padding:5px;width:17%;vertical-align:middle">
              <div class="one">
                <div class="two" id='leanagent_image'>
                  <img src='images/leanagent-1.png' width="160">
                </div>
                <img src='images/leanagent-2.png' width="160">
              </div>
              <script type="text/javascript">
                function leanagent_start() {
                  document.getElementById('leanagent_image').style.opacity = "1";
                }

                function leanagent_stop() {
                  document.getElementById('leanagent_image').style.opacity = "0";
                }
                leanagent_stop()
              </script>
            </td>
            <td style="padding:20px;width:83%;vertical-align:middle">
              <a target="_blank" href="https://leandojo.org/leanagent.html">
                <papertitle>LeanAgent: Lifelong Learning for Formal Theorem Proving</papertitle>
              </a>
              <br />
              <a target="_blank" href="https://github.com/Adarsh321123">Adarsh Kumarappan</a>, <a target="_blank" href="https://motiwari.com/">Mo Tiwari</a>, <strong>Peiyang Song</strong>, <a target="_blank" href="https://www.robertj1.com/">Robert Joseph George</a>, <a target="_blank" href="https://xiaocw11.github.io/">Chaowei Xiao</a>, and <a target="_blank" href="https://tensorlab.cms.caltech.edu/users/anima/">Anima Anandkumar</a>
              <br />
              <em>International Conference on Learning Representations (ICLR)</em>, 2025
              <br />
              <a target="_blank" href="https://arxiv.org/abs/2410.06209">arXiv</a>
			  /
			  <a target="_blank" href="https://leandojo.org/leanagent.html">project</a>
              /
              <a target="_blank" href="https://github.com/lean-dojo/LeanAgent">code</a>
              /
              <a target="_blank" href="https://iclr.cc/virtual/2025/poster/29455">proceeding</a>
			  /
			  <a target="_blank" href="https://www.marktechpost.com/2024/10/11/leanagent-the-first-life-long-learning-agent-for-formal-theorem-proving-in-lean-proving-162-theorems-previously-unproved-by-humans-across-23-diverse-lean-mathematics-repositories/">media</a>
              <p></p>
              <p>
                LeanAgent continuously learns and improves on ever-expanding mathematical knowledge without forgetting what it learned before. It has a curriculum learning strategy that optimizes the learning trajectory in terms of mathematical difficulty, a dynamic database for efficient management of evolving mathematical knowledge, and progressive training to balance stability and plasticity.
              </p>
            </td>
          </tr>

          <tr onmouseout="lean_copilot_stop()" onmouseover="lean_copilot_start()" id="leancopilot">
            <td style="padding:5px;width:17%;vertical-align:middle">
              <div class="one">
                <div class="two" id='lean_copilot_image'>
                  <img src='images/lean_copilot-1.png' width="150">
                </div>
                <img src='images/lean_copilot-2.png' width="150">
              </div>
              <script type="text/javascript">
                function lean_copilot_start() {
                  document.getElementById('lean_copilot_image').style.opacity = "1";
                }

                function lean_copilot_stop() {
                  document.getElementById('lean_copilot_image').style.opacity = "0";
                }
                lean_copilot_stop()
              </script>
            </td>
            <td style="padding:20px;width:83%;vertical-align:middle">
              <a target="_blank" href="https://leandojo.org/leancopilot.html">
                <papertitle>Lean Copilot: Large Language Models as Copilots for Theorem Proving in Lean</papertitle>
              </a>
              <br />
              <strong>Peiyang Song</strong>, <a target="_blank" href="https://yangky11.github.io/">Kaiyu Yang</a>, and <a target="_blank" href="http://tensorlab.cms.caltech.edu/users/anima/">Anima Anandkumar</a>
              <br />
              <em>International Conference on Neuro-Symbolic Systems (NeuS)</em>, 2025
              <br />
              <font color="red"><strong>1.2k+ stars on Github, ranking 2nd (only after Mathlib4) among all Lean projects</strong></font>
              <br />
              <a target="_blank" href="https://arxiv.org/abs/2404.12534">arXiv</a>
			  /
			  <a target="_blank" href="https://leandojo.org/leancopilot.html">project</a>
              /
              <a target="_blank" href="https://github.com/lean-dojo/LeanCopilot">code</a>
			  /
			  <a target="_blank" href="https://neus-2025.github.io/files/papers/paper_33.pdf">proceeding</a>
              /
              <a target="_blank" href="data/lean_copilot_poster.pdf">poster</a>
              /
              <a target="_blank" href="https://www.youtube.com/watch?v=RJJN45cjDrI">demo</a>
              /
              <a target="_blank" href="data/lean_copilot_slides.pdf">slides</a>
              /
              <a target="_blank" href="data/Neuro-Symbolic Theorem Proving with Lean.pdf">tutorial</a>
              /
              <a target="_blank" href="https://www.marktechpost.com/2024/07/30/lean-copilot-an-ai-tool-that-allows-large-language-models-llms-to-be-used-in-lean-for-proof-automation/">media</a>
              <p></p>
              <p>
                We introduce Lean Copilot, a framework for running neural network inference directly in Lean. It enables various LLM-based proof automation tools that integrate seamlessly into the workflow of Lean users, including tools for suggesting proof steps (tactics), selecting premises, and searching for complete proofs using LLMs.
              </p>
            </td>
          </tr>

          <tr onmouseout="idiom_stop()" onmouseover="idiom_start()" id="idiom">
            <td style="padding:5px;width:17%;vertical-align:middle">
              <div class="one">
                <div class="two" id='idiom_image'>
                  <img src='images/idiom-1.png' width="150">
                </div>
                <img src='images/idiom-2.png' width="150">
              </div>
              <script type="text/javascript">
                function idiom_start() {
                  document.getElementById('idiom_image').style.opacity = "1";
                }

                function idiom_stop() {
                  document.getElementById('idiom_image').style.opacity = "0";
                }
                idiom_stop()
              </script>
            </td>
            <td style="padding:20px;width:83%;vertical-align:middle">
              <a target="_blank" href="https://arxiv.org/abs/2410.00988">
                <papertitle>Creative and Context-Aware Translation of East Asian Idioms with GPT-4</papertitle>
              </a>
              <br />
              <a target="_blank" href="https://www.linkedin.com/in/kenan-tang-1682a11a0/">Kenan Tang</a>*, <strong>Peiyang Song</strong>*, <a target="_blank" href="https://yaoqin1.github.io/">Yao Qin</a>, and <a target="_blank" href="https://sites.cs.ucsb.edu/~xyan/">Xifeng Yan</a> (* <strong>Equal Contribution</strong>)
              <br />
              <em>Findings of Empirical Methods in Natural Language Processing (EMNLP)</em>, 2024
              <br />
              <a target="_blank" href="https://arxiv.org/abs/2410.00988">arXiv</a>
              /
              <a target="_blank" href="https://kenantang.github.io/cjk-idioms-gpt/">code</a>
              /
              <a target="_blank" href="https://aclanthology.org/2024.findings-emnlp.544/">proceeding</a>
			  /
			  <a target="_blank" href="https://kenantang.github.io/cjk-idioms-gpt/">demo</a>
              <p></p>
              <p>
                To compile a dictionary of East Asian idiom translations demands much time and creativity even for expert translators. To alleviate such burden, we automate high-quality data generation with GPT-4, and discover Pareto-optimal prompting strategies on both faithfulness and creativity, outperforming existing translation engines and human baseline.
              </p>
            </td>
          </tr>

          <tr onmouseout="a_not_b_stop()" onmouseover="a_not_b_start()" id="a_not_b">
            <td style="padding:5px;width:17%;vertical-align:middle">
              <div class="one">
                <div class="two" id='a_not_b_image'>
                  <img src='images/a_not_b-1.png' width="150">
                </div>
                <img src='images/a_not_b-2.png' width="150">
              </div>
              <script type="text/javascript">
                function a_not_b_start() {
                  document.getElementById('a_not_b_image').style.opacity = "1";
                }

                function a_not_b_stop() {
                  document.getElementById('a_not_b_image').style.opacity = "0";
                }
                a_not_b_stop()
              </script>
            </td>
            <td style="padding:20px;width:83%;vertical-align:middle">
              <a target="_blank" href="https://arxiv.org/abs/2409.15454">
                <papertitle>In-Context Learning May Not Elicit Trustworthy Reasoning: A-Not-B Errors in Pretrained Language Models</papertitle>
              </a>
              <br />
              <a target="_blank" href="https://pengrui-han.github.io/">Pengrui Han</a>*, <strong>Peiyang Song</strong>*, <a target="_blank" href="https://haofeiyu.me/">Haofei Yu</a>, and <a target="_blank" href="https://cs.stanford.edu/~jiaxuan/">Jiaxuan You</a> (* <strong>Equal Contribution</strong>)
              <br />
              <em>Findings of Empirical Methods in Natural Language Processing (EMNLP)</em>, 2024
              <br />
              <a target="_blank" href="https://arxiv.org/abs/2409.15454">arXiv</a>
              /
              <a target="_blank" href="https://github.com/Peiyang-Song/LLM-A-Not-B-Errors">code</a>
              /
              <a target="_blank" href="https://aclanthology.org/2024.findings-emnlp.322/">proceeding</a>
              <p></p>
              <p>
                Motivated by the crucial cognitive phenomenon of A-not-B errors, we present the first systematic evaluation on the surprisingly vulnerable inhibitory control abilities of LLMs. We reveal that this weakness undermines LLMs' trustworthy reasoning capabilities across diverse domains, and introduce various mitigations.
              </p>
            </td>
          </tr>

          <tr onmouseout="delaynet_stop()" onmouseover="delaynet_start()" id="delaynet">
            <td style="padding:5px;width:17%;vertical-align:middle">
              <div class="one">
                <div class="two" id='delaynet_image'>
                  <img src='images/delaynet-1.png' width="150">
                </div>
                <img src='images/delaynet-2.png' width="150">
              </div>
              <script type="text/javascript">
                function delaynet_start() {
                  document.getElementById('delaynet_image').style.opacity = "1";
                }

                function delaynet_stop() {
                  document.getElementById('delaynet_image').style.opacity = "0";
                }
                delaynet_stop()
              </script>
            </td>
            <td style="padding:20px;width:83%;vertical-align:middle">
              <a target="_blank" href="https://dl.acm.org/doi/10.1145/3620665.3640395">
                <papertitle>Energy Efficient Convolutions with Temporal Arithmetic</papertitle>
              </a>
              <br />
              <a target="_blank" href="https://www.linkedin.com/in/rhys-gretsch-462a951ab">Rhys Gretsch</a>, <strong>Peiyang Song</strong>, <a target="_blank" href="https://ieeexplore.ieee.org/author/38529297900">Advait Madhavan</a>, <a style="text-decoration: none" target="_blank" href="https://dl.acm.org/profile/81100206077">Jeremy Lau</a>, and <a style="text-decoration: none" target="_blank" href="https://www.arch.cs.ucsb.edu/prof-sherwood">Tim Sherwood</a>
              <br />
              <em>ACM Int'l Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)</em>, 2024
              <br />
              <a target="_blank" href="https://dl.acm.org/doi/10.1145/3620665.3640395">proceeding</a>
              <p></p>
              <p>
                We introduce energy-efficient convolution that improves the energy per pixel of each convolution frame by more than 2× compared to the state-of-the-art while improving the energy delay product by four orders of magnitude, by developing a new temporal arithmetic with a negative log transformation.
              </p>
            </td>
          </tr>

          <tr onmouseout="leandojo_stop()" onmouseover="leandojo_start()" id="leandojo">
            <td style="padding:5px;width:17%;vertical-align:middle">
              <div class="one">
                <div class="two" id='leandojo_image'>
                  <img src='images/leandojo-1.png' width="150">
                </div>
                <img src='images/leandojo-2.png' width="150">
              </div>
              <script type="text/javascript">
                function leandojo_start() {
                  document.getElementById('leandojo_image').style.opacity = "1";
                }

                function leandojo_stop() {
                  document.getElementById('leandojo_image').style.opacity = "0";
                }
                leandojo_stop()
              </script>
            </td>
            <td style="padding:20px;width:83%;vertical-align:middle">
              <a target="_blank" href="https://leandojo.org/leandojo.html">
                <papertitle>LeanDojo: Theorem Proving with Retrieval-Augmented Language Models</papertitle>
              </a>
              <br />
              <a target="_blank" href="https://yangky11.github.io/">Kaiyu Yang</a>, <a target="_blank" href="https://aidanswope.com/about">Aidan Swope</a>, <a target="_blank" href="https://minimario.github.io/">Alex Gu</a>, <a target="_blank" href="https://rchalamala.github.io/">Rahul Chalamala</a>, <strong>Peiyang Song</strong>, <a target="_blank" href="https://billysx.github.io/">Shixing Yu</a>, <a target="_blank" href="https://www.linkedin.com/in/saad-godil-9728353/">Saad Godil</a>, <a target="_blank" href="https://www.linkedin.com/in/ryan-prenger-18797ba1/">Ryan Prenger</a>, and <a target="_blank" href="http://tensorlab.cms.caltech.edu/users/anima/">Anima Anandkumar</a>
              <br />
              <em>Neural Information Processing Systems (NeurIPS), Datasets and Benchmarks Track</em>, 2023, <font color="red"><strong>Oral Presentation</strong></font>
              <br />
              <a target="_blank" href="https://arxiv.org/abs/2306.15626">arXiv</a>
              /
              <a target="_blank" href="https://leandojo.org/leandojo.html">project</a>
              /
              <a target="_blank" href="https://github.com/lean-dojo/LeanDojo">code</a>
			  /
			  <a target="_blank" href="https://zenodo.org/records/12740403">dataset</a>
			  /
			  <a target="_blank" href="https://github.com/lean-dojo/ReProver">model</a>
              /
              <a target="_blank" href="data/leandojo_poster.pdf">poster</a>
              /
              <a target="_blank" href="https://neurips.cc/virtual/2023/oral/73738">proceeding</a>
              /
              <a target="_blank" href="https://www.marktechpost.com/2023/07/01/can-llms-generate-mathematical-proofs-that-can-be-rigorously-checked-meet-leandojo-an-open-source-ai-playground-with-toolkits-benchmarks-and-models-for-large-language-models-to-prove-formal-theore/">media</a>
              <p></p>
              <p>Can LLMs generate mathematical proofs that can be rigorously checked? We release LeanDojo: an open-source playground consisting of toolkits, benchmarks, and models for LLMs to prove formal theorems in the Lean proof assistant.</p>
            </td>
          </tr>

        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Selected Media</heading>
              <p>My works have been covered by many media. Some representative ones include:</p>
              <ul>
                <li><a target="_blank" href="https://www.scientificamerican.com/article/mathematicians-newest-assistants-are-artificially-intelligent/">Mathematicians' Newest Assistants Are Artificially Intelligent</a>, Scientific American, 2024</li>
                <li><a target="_blank" href="https://www.marktechpost.com/2024/10/11/leanagent-the-first-life-long-learning-agent-for-formal-theorem-proving-in-lean-proving-162-theorems-previously-unproved-by-humans-across-23-diverse-lean-mathematics-repositories/">LeanAgent: The First Life-Long Learning Agent for Formal Theorem Proving in Lean</a>, MarkTechPost, 2024</li>
                <li><a target="_blank" href="https://www.marktechpost.com/2024/07/30/lean-copilot-an-ai-tool-that-allows-large-language-models-llms-to-be-used-in-lean-for-proof-automation/">Lean Copilot: An AI Tool that Allows Large Language Models (LLMs) to be used in Lean for Proof Automation</a>, MarkTechPost, 2024</li>
                <li><a target="_blank" href="https://www.marktechpost.com/2023/07/01/can-llms-generate-mathematical-proofs-that-can-be-rigorously-checked-meet-leandojo-an-open-source-ai-playground-with-toolkits-benchmarks-and-models-for-large-language-models-to-prove-formal-theore/">Can LLMs Generate Mathematical Proofs that can be Rigorously Checked?</a>, MarkTechPost, 2023</li>
              </ul>
            </td>
          </tr>
        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Selected Awards</heading>
              <ul>
                <li>ICLR Notable Reviewer Award (2025)</li>
                <li>George W. Housner Student Discovery Fund (2025)</li>
                <li>IEEE Micro Top Pick Award (2025)</li>
                <li>Early Research Scholarship (2023)</li>
                <li>Caltech SURF Award (2023)</li>
              </ul>
            </td>
          </tr>
        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Teaching</heading>
              <ul>
                <li><a target="_blank" href="https://www.cms.caltech.edu/academics/courses/mecsee-133-ab">ME/CS/EE 133A: Robotics - Kinematics</a>, California Institute of Technology, Fall 2025</li>
              </ul>
            </td>
          </tr>
        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Academic Services</heading>
              <ul>
                <li><strong>Reviewer</strong> @ NeurIPS, ICLR, ARR, ACL, EMNLP, IJCNLP, AACL, etc.</li>
                <li><strong>Admissions Ambassador</strong> @ Undergraduate Admissions Office, Caltech.</li>
                <li><strong>First-Year Caltech Connector</strong> (FCC) @ Student & Family Engagement Office, Caltech.</li>
                <li><strong>Organizing Staff</strong> @ Agentic AI Summit 2025, UC Berkeley.</li>
              </ul>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br />
              <p style="text-align:right;font-size:small;">
                Last updated: <b>Sep. 2025</b>. Website template credit: <a target="_blank" href="https://jonbarron.info/" style="text-align:right;font-size:small;">Jon Barron</a>.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>
</html>
